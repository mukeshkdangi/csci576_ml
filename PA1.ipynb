{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Homework 1\n",
    "\n",
    "## Instructions\n",
    "\n",
    "- Do not import other libraries. You are only allowed to use Math, Numpy, Scipy packages which are already imported in the file.\n",
    "- Please follow the type annotations. There are some type annotations of the parameters of function calls and return values. Please use Python 3.5 or 3.6 (for full support of typing annotations). You can use Numpy/Scipy inside the function.  You have to make the functions’ return values match the required type.\n",
    "- In this programming assignment you will to implement **k-Nearest Neighbours and Decision Tree**. We provide the bootstrap code and you are expected to complete the **classes** and **functions**.\n",
    "- Download all files of PA1 from Vocareum and save in the same folder.\n",
    "- Only modifications in files {`hw1_knn.py`, `hw1_dt.py`, `utils.py`} will be accepted and graded. All other modifications will be ignored. Submit those three files on Vocareum once you have finished. Which means you need to delete unnecessary files before you submit your work on Vocareum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Office Hour:\n",
    "```\n",
    "Week 2\n",
    "Jan. 14th Monday\tLVL 2nd Floor-201B\t1:00pm to 3:00pm\tCheng-Ju Lin chengjul@usc.edu\n",
    "Jan. 15th Tuesday\tLVL 2nd Floor-201B\t1:00pm to 3:00pm\tYang Fang yangfang@usc.edu\n",
    "Jan. 17th Thursday\tLVL 2nd Floor-202B\t10:00am to 12:00pm\tYixian Di yixiandi@usc.edu\n",
    "Week 3\n",
    "Jan. 22th Tuesday\tSAL 125         \t11:00am to 1:00pm\tAshir Alam ashirala@usc.edu\n",
    "Jan. 23th Wednesday\tSAL 125         \t11:00am to 1:00pm\tAshir Alam ashirala@usc.edu\n",
    "Jan. 24th Thursday\tLVL 2nd Floor-202B\t10:00am to 12:00pm\tYixian Di yixiandi@usc.edu\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: K-nearest neighbor (KNN) for binary classification (50 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some notes\n",
    "\n",
    "In this task, we will use four distance functions: (we removed the vector symbol for simplicity)\n",
    "\n",
    "- Euclidean distance:  $$d(x, y) = \\sqrt{\\langle x - y, x - y \\rangle}$$\n",
    "- Inner product distance: $$d(x, y ) = \\langle x, y \\rangle$$\n",
    "- Gaussian kernel distance: \n",
    "    $$d(x, y ) = - \\exp({−\\frac 12 \\langle x - y, x - y \\rangle}) $$\n",
    "- Cosine Distance: $$d(x, y) =\\cos(\\theta )={\\mathbf {x} \\cdot \\mathbf {y}  \\over \\|\\mathbf {x} \\|\\|\\mathbf {y} \\|}$$\n",
    "\n",
    "F1-score is a important metric for binary classification, as sometimes the accuracy metric has the false positive (a good example is in MLAPP book 2.2.3.1 “Example: medical diagnosis”, Page 29).\n",
    "We have provided a basic definition. For more you can read 5.7.2.3 from MLAPP book.\n",
    "\n",
    "<img src=\"F1Score.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.1 Distance Functions\n",
    "\n",
    "Implement the class in file *hw1_knn.py*\n",
    "    - KNN\n",
    "    \n",
    "and the functions in *utils.py*    \n",
    "    - f1_score\n",
    "    - euclidean_distance\n",
    "    - inner_product_distance\n",
    "    - gaussian_kernel_distance\n",
    "    - cosine distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from hw1_knn import KNN\n",
    "from utils import euclidean_distance, gaussian_kernel_distance, inner_product_distance, cosine_sim_distance\n",
    "from utils import f1_score\n",
    "distance_funcs = {\n",
    "    'euclidean': euclidean_distance,\n",
    "    'gaussian': gaussian_kernel_distance,\n",
    "    'inner_prod': inner_product_distance,\n",
    "    'cosine_dist': cosine_sim_distance,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data processing \n",
    "\n",
    "Do the following steps:\n",
    "\n",
    "- Load data (features and values) from function generate data_processing\n",
    "- Check that there are 303 data samples and each sample have a feature vector of length 14.\n",
    "- Split the whole data set into three parts:\n",
    "     - the train set contains 80% samples,\n",
    "     - the validation set contains the next 15% samples,\n",
    "     - the test set contains the rest 5% samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  1.00000000e+00,   6.00000000e+01,   0.00000000e+00,\n",
       "           4.00000000e+00,   1.58000000e+02,   3.05000000e+02,\n",
       "           0.00000000e+00,   2.00000000e+00,   1.61000000e+02,\n",
       "           0.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "           0.00000000e+00,   3.00000000e+00],\n",
       "        [  1.00000000e+00,   5.20000000e+01,   0.00000000e+00,\n",
       "           3.00000000e+00,   1.36000000e+02,   1.96000000e+02,\n",
       "           0.00000000e+00,   2.00000000e+00,   1.69000000e+02,\n",
       "           0.00000000e+00,   1.00000000e-01,   2.00000000e+00,\n",
       "           0.00000000e+00,   3.00000000e+00],\n",
       "        [  1.00000000e+00,   6.30000000e+01,   0.00000000e+00,\n",
       "           4.00000000e+00,   1.24000000e+02,   1.97000000e+02,\n",
       "           0.00000000e+00,   0.00000000e+00,   1.36000000e+02,\n",
       "           1.00000000e+00,   0.00000000e+00,   2.00000000e+00,\n",
       "           0.00000000e+00,   3.00000000e+00],\n",
       "        [  1.00000000e+00,   5.50000000e+01,   1.00000000e+00,\n",
       "           4.00000000e+00,   1.32000000e+02,   3.53000000e+02,\n",
       "           0.00000000e+00,   0.00000000e+00,   1.32000000e+02,\n",
       "           1.00000000e+00,   1.20000000e+00,   2.00000000e+00,\n",
       "           1.00000000e+00,   7.00000000e+00],\n",
       "        [  1.00000000e+00,   6.80000000e+01,   1.00000000e+00,\n",
       "           3.00000000e+00,   1.80000000e+02,   2.74000000e+02,\n",
       "           1.00000000e+00,   2.00000000e+00,   1.50000000e+02,\n",
       "           1.00000000e+00,   1.60000000e+00,   2.00000000e+00,\n",
       "           0.00000000e+00,   7.00000000e+00],\n",
       "        [  1.00000000e+00,   5.40000000e+01,   1.00000000e+00,\n",
       "           4.00000000e+00,   1.10000000e+02,   2.06000000e+02,\n",
       "           0.00000000e+00,   2.00000000e+00,   1.08000000e+02,\n",
       "           1.00000000e+00,   0.00000000e+00,   2.00000000e+00,\n",
       "           1.00000000e+00,   3.00000000e+00],\n",
       "        [  1.00000000e+00,   5.90000000e+01,   1.00000000e+00,\n",
       "           2.00000000e+00,   1.40000000e+02,   2.21000000e+02,\n",
       "           0.00000000e+00,   0.00000000e+00,   1.64000000e+02,\n",
       "           1.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "           0.00000000e+00,   3.00000000e+00],\n",
       "        [  1.00000000e+00,   4.00000000e+01,   1.00000000e+00,\n",
       "           4.00000000e+00,   1.10000000e+02,   1.67000000e+02,\n",
       "           0.00000000e+00,   2.00000000e+00,   1.14000000e+02,\n",
       "           1.00000000e+00,   2.00000000e+00,   2.00000000e+00,\n",
       "           0.00000000e+00,   7.00000000e+00],\n",
       "        [  1.00000000e+00,   7.10000000e+01,   0.00000000e+00,\n",
       "           4.00000000e+00,   1.12000000e+02,   1.49000000e+02,\n",
       "           0.00000000e+00,   0.00000000e+00,   1.25000000e+02,\n",
       "           0.00000000e+00,   1.60000000e+00,   2.00000000e+00,\n",
       "           0.00000000e+00,   3.00000000e+00],\n",
       "        [  1.00000000e+00,   6.00000000e+01,   0.00000000e+00,\n",
       "           4.00000000e+00,   1.50000000e+02,   2.58000000e+02,\n",
       "           0.00000000e+00,   2.00000000e+00,   1.57000000e+02,\n",
       "           0.00000000e+00,   2.60000000e+00,   2.00000000e+00,\n",
       "           2.00000000e+00,   7.00000000e+00],\n",
       "        [  1.00000000e+00,   7.00000000e+01,   1.00000000e+00,\n",
       "           3.00000000e+00,   1.60000000e+02,   2.69000000e+02,\n",
       "           0.00000000e+00,   0.00000000e+00,   1.12000000e+02,\n",
       "           1.00000000e+00,   2.90000000e+00,   2.00000000e+00,\n",
       "           1.00000000e+00,   7.00000000e+00],\n",
       "        [  1.00000000e+00,   6.50000000e+01,   1.00000000e+00,\n",
       "           4.00000000e+00,   1.35000000e+02,   2.54000000e+02,\n",
       "           0.00000000e+00,   2.00000000e+00,   1.27000000e+02,\n",
       "           0.00000000e+00,   2.80000000e+00,   2.00000000e+00,\n",
       "           1.00000000e+00,   7.00000000e+00],\n",
       "        [  1.00000000e+00,   5.70000000e+01,   1.00000000e+00,\n",
       "           4.00000000e+00,   1.50000000e+02,   2.76000000e+02,\n",
       "           0.00000000e+00,   2.00000000e+00,   1.12000000e+02,\n",
       "           1.00000000e+00,   6.00000000e-01,   2.00000000e+00,\n",
       "           1.00000000e+00,   6.00000000e+00],\n",
       "        [  1.00000000e+00,   6.20000000e+01,   1.00000000e+00,\n",
       "           3.00000000e+00,   1.30000000e+02,   2.31000000e+02,\n",
       "           0.00000000e+00,   0.00000000e+00,   1.46000000e+02,\n",
       "           0.00000000e+00,   1.80000000e+00,   2.00000000e+00,\n",
       "           3.00000000e+00,   7.00000000e+00],\n",
       "        [  1.00000000e+00,   4.30000000e+01,   0.00000000e+00,\n",
       "           3.00000000e+00,   1.22000000e+02,   2.13000000e+02,\n",
       "           0.00000000e+00,   0.00000000e+00,   1.65000000e+02,\n",
       "           0.00000000e+00,   2.00000000e-01,   2.00000000e+00,\n",
       "           0.00000000e+00,   3.00000000e+00],\n",
       "        [  1.00000000e+00,   5.90000000e+01,   1.00000000e+00,\n",
       "           4.00000000e+00,   1.10000000e+02,   2.39000000e+02,\n",
       "           0.00000000e+00,   2.00000000e+00,   1.42000000e+02,\n",
       "           1.00000000e+00,   1.20000000e+00,   2.00000000e+00,\n",
       "           1.00000000e+00,   7.00000000e+00]]),\n",
       " array([ 1.,  0.,  1.,  1.,  1.,  1.,  0.,  1.,  0.,  1.,  1.,  1.,  1.,\n",
       "         0.,  0.,  1.]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from data import data_processing\n",
    "import numpy\n",
    "Xtrain, ytrain, Xval, yval, Xtest, ytest = data_processing()\n",
    "\n",
    "Xtrain, ytrain\n",
    "Xtest, ytest\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model selection \n",
    "In kNN model, the parameter k is a hyper-parameter. In this task, we search for the best k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[part 1.1] euclidean\tk: 1\ttrain: 0.64356\tvalid: 0.56522\n",
      "[part 1.1] euclidean\tk: 3\ttrain: 0.50000\tvalid: 0.68182\n",
      "[part 1.1] euclidean\tk: 10\ttrain: 0.56436\tvalid: 0.56410\n",
      "[part 1.1] euclidean\tk: 20\ttrain: 0.37419\tvalid: 0.28571\n",
      "[part 1.1] euclidean\tk: 50\ttrain: 0.01852\tvalid: 0.00000\n",
      "\n",
      "[part 1.1] euclidean\tbest_k: 3\ttest f1 score: 0.55556\n",
      "\n",
      "[part 1.1] gaussian\tk: 1\ttrain: 0.93023\tvalid: 0.69767\n",
      "[part 1.1] gaussian\tk: 3\ttrain: 0.72727\tvalid: 0.61538\n",
      "[part 1.1] gaussian\tk: 10\ttrain: 0.68545\tvalid: 0.63415\n",
      "[part 1.1] gaussian\tk: 20\ttrain: 0.56989\tvalid: 0.45161\n",
      "[part 1.1] gaussian\tk: 50\ttrain: 0.35374\tvalid: 0.16000\n",
      "\n",
      "[part 1.1] gaussian\tbest_k: 1\ttest f1 score: 0.60000\n",
      "\n",
      "[part 1.1] inner_prod\tk: 1\ttrain: 0.61584\tvalid: 0.65625\n",
      "[part 1.1] inner_prod\tk: 3\ttrain: 0.11111\tvalid: 0.14286\n",
      "[part 1.1] inner_prod\tk: 10\ttrain: 0.00000\tvalid: 0.00000\n",
      "[part 1.1] inner_prod\tk: 20\ttrain: 0.00000\tvalid: 0.00000\n",
      "[part 1.1] inner_prod\tk: 50\ttrain: 0.00000\tvalid: 0.00000\n",
      "\n",
      "[part 1.1] inner_prod\tbest_k: 1\ttest f1 score: 0.81481\n",
      "\n",
      "[part 1.1] cosine_dist\tk: 1\ttrain: 0.60870\tvalid: 0.65625\n",
      "[part 1.1] cosine_dist\tk: 3\ttrain: 0.31628\tvalid: 0.31111\n",
      "[part 1.1] cosine_dist\tk: 10\ttrain: 0.00000\tvalid: 0.00000\n",
      "[part 1.1] cosine_dist\tk: 20\ttrain: 0.00000\tvalid: 0.00000\n",
      "[part 1.1] cosine_dist\tk: 50\ttrain: 0.00000\tvalid: 0.00000\n",
      "\n",
      "[part 1.1] cosine_dist\tbest_k: 1\ttest f1 score: 0.81481\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_k, model = KNN.model_selection_without_normalization(distance_funcs, Xtrain, ytrain, f1_score, Xval, yval, Xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification\n",
    "Here we are trying to do classification on another test set. You can make up your own data. However, we will test it on another dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "KNN.test_classify(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.2 Data transformation\n",
    "\n",
    "We are going to add one more step (data transformation) in the data processing part and see how it works. \n",
    "Sometimes, normalization plays an important role to make a machine learning model work (check term “Feature scaling” in wiki).\n",
    "\n",
    "Here, we take two different data transformation approaches.\n",
    "\n",
    "#### Normalizing the feature vector \n",
    "\n",
    "This one is simple but some times may work well. Given a feature vector $x$, the normalized feature vector is given by \n",
    "\n",
    "$$ x' = \\frac x {\\sqrt{\\langle x, x \\rangle}} $$\n",
    "If a vector is a all-zero vector, we let the normalized vector also be a all-zero vector.\n",
    "\n",
    "\n",
    "#### Min-max scaling the feature matrix\n",
    "\n",
    "The above normalization is data independent, that is to say, the output of the normalization function doesn’t depend on the rest training data. However, sometimes it would be helpful to do data dependent normalization. One thing to note is that, when doing data dependent normalization, we can only use training data, as the test data is assumed to be unknown during training (at least for most classification tasks).\n",
    "\n",
    "The min-max scaling works as follows: after min-max scaling, all values of training data’s feature vectors are in the given range.\n",
    "Note that this doesn’t mean the values of the validation/test data’s fea- tures are all in that range, because the validation/test data may have dif- ferent distribution as the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implement** the functions in *utils.py*    \n",
    "    - normalize\n",
    "    - min_max_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from utils import NormalizationScaler, MinMaxScaler\n",
    "\n",
    "scaling_classes = {\n",
    "    'min_max_scale': MinMaxScaler,\n",
    "    'normalize': NormalizationScaler,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model selection\n",
    "\n",
    "Repeat the model selection part in part 1.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_k, best_model = KNN.model_selection_with_transformation(distance_funcs,scaling_classes, Xtrain, ytrain, f1_score, Xval, yval, Xtest, ytest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading Guideline for KNN\n",
    "1. UTILS function: 15 points <br>\n",
    "\n",
    "2. 2 functions in hw1_Knn (10 points- 5 each) <br>\n",
    "\n",
    "3. Finding best K before scaling - 10 points <br>\n",
    "\n",
    "4. Finding best K after scaling - 10 points <br>\n",
    "\n",
    "5. Doing classification of the data - 5 points <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2: Decision Tree (50 points)\n",
    "- Remember from lecture, we learned that we can use decision tree to solve classification and regression problem. Mostly we focus on classification.\n",
    "- In problem 1 we used KNN to do classification. We could use decision tree algorithm to do the same job.\n",
    "- For Decision Tree, we will implement ID3 algorithm. It's garanteed that all features are discrete.\n",
    "## Part 2.1 Implementation\n",
    "### 2.1.1\n",
    "- In ID3 algorithm, we use Entropy to measure the uncertainty in the data set. We use Information Gain to measure the quality of a split.\n",
    "- Entropy: H(S)=\\\\(\\sum_{x∈X} -p(x)log_2p(x)\\\\)\n",
    "- Information_Gain: IG(S,A) = H(S)-\\\\(\\sum_{t∈T}p(t)H(T)\\\\) = H(S) - H(S|A)\n",
    "- see more detail on [ID3 Algorithm](https://en.wikipedia.org/wiki/ID3_algorithm)\n",
    "In this section, you need to implement Information_Gain function on utils.py.\n",
    "```\n",
    "def Information_Gain(branches):\n",
    "# calculate information_gain according to branches seperated by one feature\n",
    "# input:\n",
    "    -branches: List[List[int]] for a specific attribute, number of cases belongs to each attribut value and class, num_attribute_values*num_classes\n",
    "# return: float\n",
    "```\n",
    "### 2.1.2 \n",
    "- In ID3 algorithm, we use the largest information gain to split the set S.\n",
    "- Here is the pseudo code of ID3 algorithm.\n",
    "```\n",
    "Algorithm 3 The recursive procedure of decision tree algorithm\n",
    "function TREENODE.SPLIT(self)\n",
    "    ﬁnd the best attribute to split the node\n",
    "    split the node into child nodes\n",
    "    for child node in child nodes do\n",
    "        if child node.splittable then .\n",
    "            child node.split()\n",
    "        end if\n",
    "    end for\n",
    "end function\n",
    "```\n",
    "- Implement TreeNode split function and TreeNode predict function in hw1_dt.py:\n",
    "    - TreeNode.split<br>\n",
    "    **Note: when there is a tie of information gain, always choose the attribute which has more attribute values. If they are all same, use the one with small index. Also build your child list with increasing order of attribute value.**\n",
    "    - TreeNode.predict\n",
    "```\n",
    "def TreeNode.split()\n",
    "# check if current node is splitable, try to split it with all possible features\n",
    "def TreeNode.predict()\n",
    "# predic according to current node:\n",
    "# if leaf node: return current leaf node label\n",
    "# if non-leaf node: split it to child node\n",
    "```\n",
    "- Implement Decision Tree predict and train function in hw1_dt.py:\n",
    "    - DecisionTree.train\n",
    "    - DecisionTree.predict\n",
    "\n",
    "```\n",
    "def DecisionTree.train(features, labels)\n",
    "# train decision tree based on training dataset, grow your decision tree from root node\n",
    "# input: \n",
    "    - features: List[List[any]] traning data, num_cases*num_attributes\n",
    "    - labels: List[any] traning labels, num_cases*1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.2 Sanity Test\n",
    "Do the following steps, as a simple test to check your algorithm works well\n",
    "- Load training data (features and values) from function data.sample_decision_tree_data.\n",
    "- Create a Decision Tree based on training data.\n",
    "- Load test data from function data.sample_decision_tree_test.\n",
    "- Test the prediction function of your algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import data\n",
    "import hw1_dt as decision_tree\n",
    "import utils as Utils\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "features, labels = data.sample_decision_tree_data()\n",
    "\n",
    "# build the tree\n",
    "dTree = decision_tree.DecisionTree()\n",
    "dTree.train(features, labels)\n",
    "\n",
    "# print\n",
    "Utils.print_tree(dTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data\n",
    "X_test, y_test = data.sample_decision_tree_test()\n",
    "\n",
    "# testing\n",
    "y_est_test = dTree.predict(X_test)\n",
    "test_accu = accuracy_score(y_est_test, y_test)\n",
    "print('test_accu', test_accu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.3 Train and Predict\n",
    "### 2.3.1\n",
    "- Load data (features and values) from function data.load_decision_tree_data.\n",
    "- Train your decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load data\n",
    "X_train, y_train, X_test, y_test = data.load_decision_tree_data()\n",
    "\n",
    "# set classifier\n",
    "dTree = decision_tree.DecisionTree()\n",
    "\n",
    "# training\n",
    "dTree.train(X_train.tolist(), y_train.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2\n",
    "- Print your decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print\n",
    "Utils.print_tree(dTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.3\n",
    "- do prediction on test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "# testing\n",
    "y_est_test = dTree.predict(X_test)\n",
    "test_accu = accuracy_score(y_est_test, y_test)\n",
    "print('test_accu', test_accu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.4 Pruning The Tree\n",
    "Sometimes, in order to prevent overfitting. We need to pruning our Decition Tree. There are several approaches to avoiding overfitting in building decision trees. \n",
    "\n",
    "- Pre-pruning that stop growing the tree earlier, before it perfectly classifies the training set.\n",
    "- Post-pruning that allows the tree to perfectly classify the training set, and then post prune the tree. \n",
    "\n",
    "Practically, the second approach of post-pruning overfit trees is more successful because it is not easy to precisely estimate when to stop growing the tree.\n",
    "We will use Reduced Error Pruning, as one of Post-pruning in this part.\n",
    "```\n",
    "Reduced Error Pruning\n",
    "0. Split data into training and validation sets.\n",
    "1. Do until further pruning is harmful:\n",
    "2. Evaluate impact on validation set of pruning each possible node (plus those below it)\n",
    "3. Greedily remove the one that most improves validation set accuracy\n",
    "- Produces smallest version of most accurate subtree.\n",
    "- Requires that a lot of data be available.\n",
    "```\n",
    "For Pruning of Decision Tree, you can refer [Reduce Error Pruning](http://jmvidal.cse.sc.edu/talks/decisiontrees/reducederrorprun.html?style=White) and P69 of Textbook: Machine Learning -Tom Mitchell.\n",
    "\n",
    "### 2.4.1 \n",
    "**Hint: in this part, you can add another parameters or functions in TreeNode class and DecisionTree class for your convenience. But your changes should not influent results of previous parts.**<br>\n",
    "implement the reduced_error_pruning function on util.py.\n",
    "\n",
    "```\n",
    "def reduced_error_pruning(decitionTree):\n",
    "# input: \n",
    "    - decitionTree: decitionTree trained based on training data set.\n",
    "    - X_test: List[List[any]] test data, num_cases*num_attributes\n",
    "    - y_test: List[any] test labels, num_cases*1\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Utils.reduced_error_pruning(dTree, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.2\n",
    "Test your prediction accuracy on validation dataset after pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_est_test = dTree.predict(X_test)\n",
    "test_accu = accuracy_score(y_est_test, y_test)\n",
    "print('test_accu', test_accu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.3\n",
    "Print your decision tree after pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Utils.print_tree(dTree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grading Guidline\n",
    "1. Information_Gain function - 10 points <br>\n",
    "we will test your Infomation Gain function on another ten inputs. To receive full credits of this part, your function should be able to output right valus.\n",
    "2. Train your decision tree - 15 points <br>\n",
    "we will test your decision tree after training on training dataset. To receive full credit of this part, your algorithm will generate the identical decision tree as our answer.\n",
    "3. Prediction of decision tree - 10 points <br>\n",
    "we will use another dataset to test your prediction part of decision tree, you can assume that test dataset has identical attributs and values as traning dataset. To receive full credit of this part, your algorithm will generate the identical prediction of our answer.\n",
    "4. Pruning of decision tree - 15 points <br>\n",
    "we will test your decision tree after pruning. To receive full credit of this part, your algorithm will generate the identical decision tree as our answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good Luck! : )"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
